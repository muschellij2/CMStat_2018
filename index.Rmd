---
title: "My First Exposure to Accelerometer Data was for 100000 People from UK Biobank"
subtitle: "John Muschelli, \\@StrictlyStat,\n Johns Hopkins University"
author: "presentation: http://johnmuschelli.com/CMStat_2018"
date: "source: http://github.com/muschellij2/CMStat_2018"
output:
  ioslides_presentation:
    css: CMStat_2018.css
    widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE)
library(readr)
library(knitr)
library(kableExtra)
options("kableExtra.html.bsTable" = TRUE)
```


## UK Biobank Data

- 500,000 participants
- 100,000 inlucded in the sub-study
- Can infer movement with longitudinal follow-up

<!-- Do you move meme -->

## Data Gathered

- Tri-axial Axtivity 100Hz over 7 days
- Started at 10AM and ended at 10PM
- Data measured in milli-g (1$g$ = 9.80665 $\frac{m}{s^2}$)
  - not counts or steps as other devices

# The devil is (or can be) in the inclusion criteria
  
  
## UK Biobank Data

```{r, out.width="70%", fig.align="center"}
knitr::include_graphics("images/exclusion_criteria.png")
```


## Demographics: Lots of Non-Response

```{r}
df = read_csv("summary/accel_stratified_summary_stats.csv", na = "NA")
df = df[c(1:4),]
df =  as.data.frame(df)
df[2, 2:5] = gsub("[.](\\d)\\d", ".\\1", df[2,2:5])
rows = which(df[, 1, drop = TRUE] %in% c("Normal", "Underweight", 
                     "Overweight", "Obese",  "Good", 
                     "Poor", "Fair", "Excellent", 
                     "Never", "Former", "Current"))
colnames(df)[1] = ""
knitr::kable(df) %>% 
  kable_styling("striped") %>%
  add_indent(rows)
```

**Many people DIED before being able to be asked**

## Assessment to Accelerometry can be a WHILE

```{r, out.width="100%", fig.align="center"}
knitr::include_graphics("images/time_diff.png")
```



## Responders are Healthier (Self-Reported)

```{r}
df = read_csv("summary/accel_stratified_summary_stats.csv", na = "NA")
df = df[c(1:3, 14:18),]
df[ 5:8, ] = df[c(8, 5, 7, 6), ]
df = df[4:8,]
df =  as.data.frame(df)
rows = which(df[, 1, drop = TRUE] %in% c("Normal", "Underweight", 
                     "Overweight", "Obese",  "Good", 
                     "Poor", "Fair", "Excellent", 
                     "Never", "Former", "Current"))
colnames(df)[1] = ""
knitr::kable(df) %>% 
  kable_styling("striped") %>%
  add_indent(rows) %>% 
  row_spec(2, bold = TRUE)
```
  
## Accelerometry Data Available

- Data at varyling levels
  - Axtivity CWA format (Highest resolution, 100Hz)
    - very large for 100K subjects (Estimate size)
- 5 second level data
  - imputation/processing done
  - averaged into minute-level data
- Overall statistics (mean/median): overall, daily, hourly, day of week
  - removed "non-wear" periods
  
## Accelerometry Data Available

- Data at varyling levels
  - Axtivity CWA format (Highest resolution, 100Hz)
    - very large for 100K subjects
- **5 second level data**
  - *imputation/processing done*
  - **averaged into minute-level data**
- Overall statistics (mean/median): overall, daily, hourly, day of week
  - removed "non-wear" periods  
  

## Processing done (not by me)

Auto-calibration [@van2014autocalibration]

- 10$s$ window all axes SD $< 13.0$ m$g$.
- fit a unit gravity sphere using OLS.
- If 3 axes had values outside a $\pm 300$m$g$ range - use calibration coefficient from that person
- if not use next peron's accelerometer record from the same device

<!-- Show some of the things about device specific measures -->
<!-- - no large signs of bias -->
<!-- - plan on doing matching -->

# Lesson #1: If magnitude is important, need calibration ("batch effect" correction)

<!-- Let me get Some of that data meme -->

## Processing done (not by me)

- recording errors and 'interrupts' flagged (plug in accelerometer)
- $\pm8g$ flagged
- Resampled to 100 Hz (interrupts > 5 seconds set to missing)
- Euclidean norm, fourth order Butterworth low pass filter (f = 20Hz). 
- Subtract $1g$, negative values set to $0$

@doherty2017large

# At least they have software to do this! (Written in LANGUAGE?)
<!-- PIC HERE -->

## Goal of analysis

- Explore the data 
- Get similar findings to some [@doherty2017large] analysis
- Assess "bias" in different devices (see if autocalibration is working)


```{r, out.width="70%", fig.align="center"}
knitr::include_graphics("images/plos_age_plot.png")
```

## How do people typically move? (First plot)

- Average all data - regardless of mulitple visits per person

```{r, eval = FALSE, out.width="70%", fig.align="center"}
knitr::include_graphics("images/overall_image.png")
```


## How do people typically move? (First plot)
<!-- PUT BOX AROUND WEIRD AREAS -->
- Average all data - regardless of mulitple visits per person

```{r, eval = FALSE, out.width="70%", fig.align="center"}
knitr::include_graphics("images/overall_image.png")
```

1.  Run full - show weird shit
2.  Run only 95% data - show how some goes away
  - say maybe solved
3.  Run by day - show all 7 and the weird things
4.  Show how threshold throws out day 0 and 7 (probably good thing) - peak problem
5.  Show the jumps still
6.  Well maybe it's the mean - just use the median - should fix it - nope
7.  Show 1-2 raw data (Need this)

## How do people typically move? 

- Only keep when 95% (1368 minutes) are not missing

```{r, eval = FALSE, out.width="70%", fig.align="center"}
knitr::include_graphics("images/overall_subgood_image.png")
```

# Maybe Lesson #2: Keep only full days

## Not so fast, let's look day by day

- Take Day 0 - 7, day 0 is when they started wearing it
  - this is what we saw in the data though it should be 0-6
- Calculate statistic over all participants
  - the same minute across people
  - each day separately
  

## Randomly Sample 2000 people

```{r, out.width="70%", fig.align="center"}
knitr::include_graphics("images/unclustered_log10_heatmap_no_imputed_threshold_plot.png")
```

## Maybe it's a "few bad apples"

```{r, out.width="70%", fig.align="center"}
knitr::include_graphics("images/plos_age_plot.png")
```




# We're ready for analysis right?

- Nope!  Need to estimate "wear time" 

<!-- A ghost wore it meme - like who's wearing that sheet? -->
<!--  Maybe set it and forget it? -->

## Wear time

"We removed non-wear time, defined as consecutive stationary episodes lasting for at least 60 minutes where all three axes had a standard deviation of less than 13.0 m$g$"

@doherty2017large

<!-- OK meme?  Maybe set it and forget it? -->

## Why 13m$g$?

"Here, 13 m$g$ was selected just above the empirically derived baseline (noise) standard deviation of 10 m$g$ to retain only nonmovement periods." @van2014autocalibration

<!-- African kid skeptical meme with  -->

## Takehome Messages

1. Start off smaller than 100K people
2. Inspect the raw(ish) data
3. Autocalibration seems to work well on gross features (with 100K people)
4. Artifacts still seem present in the data
5. **Back to the 100Hz data** we go!

## Next data installment 

"We invited some participants to wear an activity monitor for a week, four times a year. ... finished in early 2019."

<!-- NO MORE DATA -->
